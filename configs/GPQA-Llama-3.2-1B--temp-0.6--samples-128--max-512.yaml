model:
  model_id: meta-llama/Llama-3.2-1B-Instruct
  device: cuda
  dtype: bfloat16

experiment:
  max_tokens: 512
  temperature: 0.6
  n_samples: 128
  output_path: ../final_runs/GPQA/Llama-3.2-1B--temp-0.6--samples-128--max-512.jsonl

  reasoning: false

benchmark:
  type: gsm8k_jsonl
  params:
    path: ../datasets/GPQA/test.jsonl

verifier:
  type: gpqa_mc
